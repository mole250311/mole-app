from flask import Blueprint, request, jsonify
import os
import requests
import time
import xml.etree.ElementTree as ET
import nltk
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx
from typing import List
import calendar
from deep_translator import GoogleTranslator
paper_route = Blueprint("papers", __name__)

# ------------------- NLTK ì´ˆê¸°í™” -------------------
NLTK_DATA_PATH = os.path.expanduser("~/nltk_data")
nltk.data.path = [NLTK_DATA_PATH]

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt", download_dir=NLTK_DATA_PATH)

# ------------------- ìš”ì•½ í•¨ìˆ˜ -------------------
def summarize_abstract(abstract: str, num_sentences: int = 2) -> str:
    try:
        sentences = sent_tokenize(abstract)
        if len(sentences) < 2:
            return abstract
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(sentences)
        sim_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()
        graph = nx.from_numpy_array(sim_matrix)
        scores = nx.pagerank(graph)
        ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
        return " ".join([s for _, s in ranked_sentences[:num_sentences]])
    except Exception as e:
        print(f"[âš ï¸ ìš”ì•½ ì˜¤ë¥˜] {e}")
        return abstract

# ------------------- í‚¤ì›Œë“œ ì¶”ì¶œ -------------------
def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    try:
        if not text or len(text.strip()) < 10:
            return []
        vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform([text])
        scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray()[0])
        ranked = sorted(scores, key=lambda x: x[1], reverse=True)
        return [word for word, _ in ranked[:top_n]]
    except Exception as e:
        print(f"[âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜] {e}")
        return []

# ------------------- ë…¼ë¬¸ ê²€ìƒ‰ í•¨ìˆ˜ -------------------
def fetch_papers(query: str, retmax: int = 5) -> List[dict]:
    search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    search_params = {"db": "pubmed", "term": query, "retmax": retmax, "retmode": "json"}
    response = requests.get(search_url, params=search_params)
    id_list = response.json().get("esearchresult", {}).get("idlist", [])

    print("[ğŸ§ª] PMID list:", id_list)

    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    papers = []
    month_map = {name: f"{num:02d}" for num, name in enumerate(calendar.month_abbr) if name}

    for pmid in id_list:
        fetch_params = {"db": "pubmed", "id": pmid, "retmode": "xml"}
        try:
            fetch_response = requests.get(fetch_url, params=fetch_params)
            fetch_response.raise_for_status()
            root = ET.fromstring(fetch_response.text)

            for article in root.findall("PubmedArticle"):
                article_data = article.find(".//Article")
                if article_data is None:
                    continue

                title_en = article_data.findtext("ArticleTitle", default="(No Title)")
                abstract_nodes = article_data.findall(".//AbstractText")
                abstract = " ".join(["".join(node.itertext()).strip() for node in abstract_nodes if node is not None])
                journal = article_data.findtext(".//Journal/Title", default="Unknown Journal")

                pub_date_elem = article.find(".//PubDate")
                if pub_date_elem is not None:
                    year = pub_date_elem.findtext("Year", "")
                    month_raw = pub_date_elem.findtext("Month", "")
                    day = pub_date_elem.findtext("Day", "01")
                else:
                    year = ""
                    month_raw = ""
                    day = "01"

                month = month_map.get(month_raw[:3].capitalize(), "01") if month_raw else "01"
                pub_date = f"{year}-{month}-{day}" if year else "Unknown Date"

                article_types = [e.text for e in article.findall(".//PublicationType") if e.text] or ["Unknown Type"]
                pages = article.findtext(".//Pagination/MedlinePgn", default="")

                authors = []
                for author in article_data.findall(".//Author"):
                    first = author.findtext("ForeName")
                    last = author.findtext("LastName")
                    if first and last:
                        authors.append(f"{first} {last}")
                authors_str = authors if authors else ["Unknown Author"]

                summary = summarize_abstract(abstract)
                keywords = extract_keywords(abstract)

                papers.append({
                    "title_en": title_en,
                    "abstract": summary,
                    "abstract_full": abstract,
                    "keywords": keywords,
                    "link": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                    "authors": authors_str,
                    "journal": journal,
                    "pub_date": pub_date,
                    "pages": pages
                })

                print(f"[âœ…] PMID {pmid} ì²˜ë¦¬ ì™„ë£Œ: {title_en}")
                time.sleep(0.3)

        except Exception as e:
            print(f"[âš ï¸] PMID {pmid} ì—ëŸ¬: {e}")
            continue

    return papers


# ë²ˆì—­ í•¨ìˆ˜
def translate(text):
    try:
        return GoogleTranslator(source='en', target='ko').translate(text)
    except Exception as e:
        print(f"[ë²ˆì—­ ì˜¤ë¥˜] {e}")
        return "(ë²ˆì—­ ì‹¤íŒ¨)"


# ë…¼ë¬¸ ë²ˆì—­ API ì—”ë“œí¬ì¸íŠ¸
def translate_paper(query: str, retmax: int) -> List[dict]:
    search_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    search_params = {"db": "pubmed", "term": query, "retmax": retmax, "retmode": "json"}

    try:
        response = requests.get(search_url, params=search_params)
        response.raise_for_status()
        id_list = response.json().get("esearchresult", {}).get("idlist", [])
    except Exception as e:
        return jsonify({"error": f"PubMed ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}"}), 500

    fetch_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
    results = []
    month_map = {name: f"{num:02d}" for num, name in enumerate(calendar.month_abbr) if name}

    for pmid in id_list:
        try:
            fetch_params = {"db": "pubmed", "id": pmid, "retmode": "xml"}
            fetch_res = requests.get(fetch_url, params=fetch_params)
            fetch_res.raise_for_status()

            root = ET.fromstring(fetch_res.text)
            article = root.find(".//PubmedArticle")
            art_data = article.find(".//Article") if article is not None else None

            if art_data is None:
                continue

            # ì œëª©
            title_en = art_data.findtext("ArticleTitle", default="(No Title)")
            title_ko = translate(title_en)

            # ë³¸ë¬¸(ì¶”ì¶œ)
            abstract_nodes = art_data.findall(".//AbstractText")
            abstract_en = " ".join(["".join(node.itertext()).strip() for node in abstract_nodes if node is not None])
            abstract_ko = translate(abstract_en)

            # ì €ì
            authors_en = []
            authors_ko = []
            for author in art_data.findall(".//Author"):
                first = author.findtext("ForeName")
                last = author.findtext("LastName")
                if first and last:
                    full = f"{first} {last}"
                    authors_en.append(full)
                    authors_ko.append(translate(full))

            # ìœ í˜•
            type_list = article.findall(".//PublicationType")
            type = type_list[0].text if type_list else "ìœ í˜• ì—†ìŒ"

            # ì €ë„
            journal = art_data.findtext(".//Journal/Title", default="Unknown Journal")

            # ë°œí–‰ì¼
            pub_date_elem = article.find(".//PubDate")
            if pub_date_elem is not None:
                year = pub_date_elem.findtext("Year", "")
                month_raw = pub_date_elem.findtext("Month", "")
                day = pub_date_elem.findtext("Day", "01")
            else:
                year = ""
                month_raw = ""
                day = "01"
            month = month_map.get(month_raw[:3].capitalize(), "01") if month_raw else "01"
            pub_date = f"{year}-{month}-{day}" if year else "Unknown Date"

            # í˜ì´ì§€
            pages = article.findtext(".//Pagination/MedlinePgn", default="")

            results.append({
                "pmid": pmid,
                "title_en": title_en,
                "title_ko": title_ko,
                "abstract_en": abstract_en,
                "abstract_ko": abstract_ko,
                "authors_en": authors_en,
                "authors_ko": authors_ko,
                "type": type,
                "journal": journal,
                "pub_date": pub_date,
                "pages": pages,
                "link": f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
            })

            print(f"[âœ…] PMID {pmid} ë²ˆì—­ í¬í•¨ ì™„ë£Œ")
            time.sleep(0.3)

        except Exception as e:
            print(f"[âš ï¸] PMID {pmid} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    return results

# ------------------- ì—”ë“œí¬ì¸íŠ¸ -------------------
@paper_route.route("/papers", methods=["GET"])
def get_papers():
    query = request.args.get("query")
    if not query:
        return jsonify({"error": "Query parameter required"}), 400
    try:
        papers = translate_paper(query=query, retmax=5)
        return jsonify({"query": query, "papers": papers})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
